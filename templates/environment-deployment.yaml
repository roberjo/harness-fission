template:
  name: environment-deployment
  identifier: environment_deployment
  type: Stage
  spec:
    variables:
      - name: environment
        type: String
        required: true
      - name: workspace
        type: String
        required: true
      - name: requireApproval
        type: Boolean
        default: false
      - name: minApprovers
        type: Number
        default: 1
      - name: jmeterUsers
        type: Number
        default: 10
        description: "Number of concurrent users for load test"
      - name: jmeterRampUp
        type: Number
        default: 60
        description: "Ramp-up period in seconds"
      - name: jmeterDuration
        type: Number
        default: 300
        description: "Test duration in seconds"
      - name: jmeterP95Threshold
        type: Number
        default: 2000
        description: "95th percentile response time threshold in milliseconds"
      - name: jmeterErrorThreshold
        type: Number
        default: 1
        description: "Maximum allowed error rate percentage"
      - name: sonarQubeGateTimeout
        type: Number
        default: 300
        description: "Timeout in seconds for SonarQube quality gate"
      - name: zapThreshold
        type: String
        default: "MEDIUM"
        description: "Minimum severity level for ZAP findings to fail the pipeline"
      - name: snykSeverity
        type: String
        default: "high"
        description: "Minimum severity level for Snyk findings to fail the pipeline"
      - name: unitTestCoverage
        type: Number
        default: 80
        description: "Minimum unit test coverage percentage required"
      - name: testResultPath
        type: String
        default: "test-results"
        description: "Path to store test results"
      - name: notificationChannels
        type: String
        default: "slack,email"
        description: "Comma-separated list of notification channels"
      - name: monitoringDuration
        type: Number
        default: 900
        description: "Post-deployment monitoring duration in seconds"

    stage:
      name: <+variables.environment>
      identifier: <+variables.environment>
      description: <+variables.environment> environment deployment
      type: Deployment
      spec:
        serviceConfig:
          serviceRef: <+input>
          serviceDefinition:
            type: Terraform
            spec:
              configuration:
                type: TerraformCloud
                spec:
                  workspace: <+variables.workspace>
                  organizationId: <+org.variables.TF_CLOUD_ORG>
        infrastructure:
          environmentRef: <+variables.environment>
          infrastructureDefinition:
            type: TerraformCloud
            spec:
              workspace: <+variables.workspace>
              organizationId: <+org.variables.TF_CLOUD_ORG>
        execution:
          steps:
            - step:
                name: Vault Authentication
                identifier: vault_auth
                type: HarnessApproval
                spec:
                  execution:
                    steps:
                      - step:
                          type: Shell
                          name: Get Vault Token
                          identifier: get_vault_token
                          spec:
                            shell: Bash
                            command: |
                              export VAULT_TOKEN=$(curl -X POST -H "X-Vault-Request: true" -d '{"role_id":"${VAULT_ROLE_ID}","secret_id":"${VAULT_SECRET_ID}"}' ${VAULT_ADDR}/v1/auth/approle/login | jq -r '.auth.client_token')
                              harness secret update VAULT_TOKEN_<+variables.environment> -v $VAULT_TOKEN

            - step:
                name: Get AWS Credentials
                identifier: get_aws_creds
                type: Shell
                spec:
                  command: |
                    export AWS_CREDS=$(curl -H "X-Vault-Token: ${VAULT_TOKEN_<+variables.environment>}" ${VAULT_ADDR}/v1/aws/creds/terraform-role)
                    harness secret update AWS_ACCESS_KEY_<+variables.environment> -v $(echo $AWS_CREDS | jq -r '.data.access_key')
                    harness secret update AWS_SECRET_KEY_<+variables.environment> -v $(echo $AWS_CREDS | jq -r '.data.secret_key')

            - step:
                name: SonarQube Analysis
                identifier: sonar_scan
                type: Plugin
                spec:
                  connectorRef: sonarqube
                  image: sonarsource/sonar-scanner-cli:latest
                  command: |
                    sonar-scanner \
                      -Dsonar.projectKey=${PROJECT_NAME} \
                      -Dsonar.sources=. \
                      -Dsonar.host.url=${SONAR_HOST_URL} \
                      -Dsonar.login=${SONAR_TOKEN}

            - step:
                name: SonarQube Quality Gate
                identifier: sonar_gate
                type: Barrier
                spec:
                  execution:
                    steps:
                      - step:
                          type: Plugin
                          name: Check Quality Gate
                          identifier: check_quality_gate
                          spec:
                            connectorRef: sonarqube
                            image: curlimages/curl:latest
                            command: |
                              ANALYSIS_ID=$(curl -s -u "${SONAR_TOKEN}:" "${SONAR_HOST_URL}/api/ce/component?component=${PROJECT_NAME}" | jq -r '.current.id')
                              STATUS=$(curl -s -u "${SONAR_TOKEN}:" "${SONAR_HOST_URL}/api/qualitygates/project_status?analysisId=${ANALYSIS_ID}" | jq -r '.projectStatus.status')
                              if [ "$STATUS" != "OK" ]; then
                                echo "Quality gate failed with status: $STATUS"
                                exit 1
                  barrier:
                    name: Quality Gate
                    timeout: <+variables.sonarQubeGateTimeout>

            - step:
                name: Container Image Scan
                identifier: trivy_scan
                type: Plugin
                spec:
                  connectorRef: docker
                  image: aquasec/trivy:latest
                  command: |
                    trivy image ${APP_IMAGE} \
                      --exit-code 1 \
                      --severity HIGH,CRITICAL \
                      --no-progress

            - step:
                name: Dependency Security Scan
                identifier: snyk_scan
                type: Plugin
                spec:
                  connectorRef: snyk
                  image: snyk/snyk:latest
                  command: |
                    snyk test \
                      --severity-threshold=<+variables.snykSeverity> \
                      --all-projects

            - step:
                name: Run Unit Tests
                identifier: unit_tests
                type: Run
                failureStrategies:
                  - onFailure:
                      errors: [AllErrors]
                      action: StageRollback
                spec:
                  command: |
                    # Run tests with coverage
                    pytest --cov=src --cov-report=xml --cov-report=html --junitxml=unit-tests.xml
                    
                    # Check coverage threshold
                    COVERAGE=$(coverage report | grep TOTAL | awk '{print $4}' | sed 's/%//')
                    if [ "${COVERAGE}" -lt "<+variables.unitTestCoverage>" ]; then
                      echo "Coverage ${COVERAGE}% below threshold of <+variables.unitTestCoverage>%"
                      exit 1
                    fi

            - step:
                name: Integration Tests
                identifier: integration_tests
                type: Run
                spec:
                  command: |
                    # Run integration tests
                    newman run collections/integration-tests.json \
                      --environment environments/<+variables.environment>.json \
                      --reporters cli,junit,htmlextra \
                      --reporter-junit-export integration-tests.xml \
                      --reporter-htmlextra-export integration-report.html

            - step:
                name: Terraform Plan
                identifier: terraform_plan
                type: TerraformPlan
                spec:
                  configuration:
                    type: TerraformCloud
                    workspace: <+variables.workspace>
                    varFiles:
                      - <+variables.environment>.tfvars
                  environmentVariables:
                    - name: AWS_ACCESS_KEY_ID
                      value: <+secrets.getValue("AWS_ACCESS_KEY_<+variables.environment>")>
                    - name: AWS_SECRET_ACCESS_KEY
                      value: <+secrets.getValue("AWS_SECRET_KEY_<+variables.environment>")>

            - step:
                name: WIZ IaC Scan
                identifier: wiz_scan
                type: Run
                spec:
                  command: |
                    wiz-cli auth --id ${WIZ_CLIENT_ID} --secret ${WIZ_CLIENT_SECRET}
                    wiz-cli iac scan -p terraform.tfplan --policy policy.yaml

            - step:
                name: Terraform Apply
                identifier: terraform_apply
                type: TerraformApply
                spec:
                  configuration:
                    type: TerraformCloud
                    workspace: <+variables.workspace>

            - step:
                name: OWASP ZAP Scan
                identifier: zap_scan
                type: Plugin
                spec:
                  connectorRef: docker
                  image: owasp/zap2docker-stable:latest
                  command: |
                    zap-baseline.py \
                      -t ${APP_URL} \
                      -c gen.conf \
                      -I \
                      -l WARN \
                      -T 10 \
                      -m 5 \
                      --hook=/zap/auth.py \
                      --auto

            - step:
                name: Seeker Security Scan
                identifier: seeker_scan
                type: Plugin
                spec:
                  connectorRef: seeker
                  image: seeker-agent:latest
                  command: |
                    seeker-agent scan --url ${APP_URL} --project ${PROJECT_NAME}

            - step:
                name: Setup JMeter Test Resources
                identifier: setup_jmeter_resources
                type: Run
                spec:
                  command: |
                    # Create directories for JMeter artifacts
                    mkdir -p jmeter/tests jmeter/results

                    # Copy test plans and supporting files
                    cp load-tests/<+variables.environment>/*.jmx jmeter/tests/
                    cp load-tests/<+variables.environment>/test-data/* jmeter/tests/ || true
                    
                    # Validate test plan exists
                    if [ ! -f "jmeter/tests/load-test.jmx" ]; then
                      echo "Error: JMeter test plan not found"
                      exit 1

            - step:
                name: JMeter Load Test
                identifier: jmeter_test
                type: Plugin
                spec:
                  connectorRef: jmeter
                  image: jmeter:latest
                  command: |
                    # Set JMeter properties
                    export JVM_ARGS="-Xmx1024m -Djava.security.egd=file:/dev/urandom"
                    export JMETER_OPTS="-Jjmeter.save.saveservice.timestamp_format=yyyy-MM-dd HH:mm:ss"
                    
                    # Run JMeter test with comprehensive settings
                    jmeter \
                      -n \
                      -t jmeter/tests/load-test.jmx \
                      -l jmeter/results/results.jtl \
                      -e -o jmeter/results/dashboard \
                      -Jusers=<+variables.jmeterUsers|10> \
                      -Jrampup=<+variables.jmeterRampUp|60> \
                      -Jduration=<+variables.jmeterDuration|300> \
                      -Jtarget_env=<+variables.environment> \
                      -Jbase_url=${APP_URL} \
                      -Jthreads.group1.threads=<+variables.jmeterUsers|10> \
                      -Jthreads.group1.rampup=<+variables.jmeterRampUp|60> \
                      -Jjmeter.save.saveservice.timestamp_format=yyyy-MM-dd HH:mm:ss \
                      -Jjmeter.save.saveservice.output_format=csv \
                      -Jjmeter.save.saveservice.response_data=false \
                      -Jjmeter.save.saveservice.samplerData=false \
                      -Jjmeter.save.saveservice.requestHeaders=false \
                      -Jjmeter.save.saveservice.responseHeaders=false \
                      || exit 1

            - step:
                name: Analyze Load Test Results
                identifier: analyze_load_test
                type: Run
                failureStrategies:
                  - onFailure:
                      errors:
                        - AllErrors
                      action:
                        type: StageRollback
                spec:
                  command: |
                    # Run analysis script with error handling
                    if [ -f "jmeter/results/results.jtl" ]; then
                      python analyze_sla.py \
                        --results-file jmeter/results/results.jtl \
                        --environment <+variables.environment> \
                        --threshold-p95 <+variables.jmeterP95Threshold|2000> \
                        --threshold-error-rate <+variables.jmeterErrorThreshold|1>
                      
                      # Check exit code and set barrier variables
                      if [ $? -eq 0 ]; then
                        echo "Load test passed all thresholds"
                        harness workflow variable set LOAD_TEST_STATUS "passed"
                      else
                        echo "Load test failed to meet performance thresholds"
                        harness workflow variable set LOAD_TEST_STATUS "failed"
                        exit 1
                      fi
                    else
                      echo "Error: JMeter results file not found"
                      harness workflow variable set LOAD_TEST_STATUS "failed"
                      exit 1
                    fi

            - step:
                name: Verify Performance Gate
                identifier: verify_performance_gate
                type: Barrier
                spec:
                  execution:
                    steps:
                      - step:
                          type: Shell
                          name: Check Performance Results
                          identifier: check_performance
                          spec:
                            shell: Bash
                            command: |
                              if [ "<+workflow.variables.LOAD_TEST_STATUS>" = "failed" ]; then
                                echo "Performance gate check failed - Load test results did not meet thresholds"
                                exit 1
                              else
                                echo "Performance gate check passed"
                              fi
                barrier:
                  name: Performance Gate
                  dependencies:
                    - analyze_load_test
                  propagateFailure: true
                  evaluateConditions:
                    - condition: <+workflow.variables.LOAD_TEST_STATUS> == "passed"
                      message: "Load test results meet performance requirements"
                    
            - step:
                name: Aggregate Test Results
                identifier: test_results
                type: Run
                spec:
                  command: |
                    mkdir -p <+variables.testResultPath>/<+variables.environment>
                    
                    # Copy all test results
                    cp unit-tests.xml <+variables.testResultPath>/<+variables.environment>/
                    cp -r htmlcov <+variables.testResultPath>/<+variables.environment>/coverage
                    cp integration-tests.xml <+variables.testResultPath>/<+variables.environment>/
                    cp integration-report.html <+variables.testResultPath>/<+variables.environment>/
                    cp -r jmeter/results/* <+variables.testResultPath>/<+variables.environment>/performance/
                    
                    # Generate summary report
                    python scripts/generate_test_summary.py \
                      --unit-test unit-tests.xml \
                      --integration-test integration-tests.xml \
                      --performance-test jmeter/results/results.jtl \
                      --output <+variables.testResultPath>/<+variables.environment>/summary.html

            - step:
                name: Upload Test Results
                identifier: upload_test_results
                type: Upload
                spec:
                  files:
                    - jmeter/results/**/*
                  connectorRef: artifacts_connector
                  metadata:
                    environment: <+variables.environment>
                    testType: performance
                    timestamp: ${PIPELINE_EXECUTION_ID}

            - step:
                name: Upload Test Reports
                identifier: upload_reports
                type: Upload
                spec:
                  files:
                    - <+variables.testResultPath>/<+variables.environment>/**/*
                  connectorRef: artifacts_connector
                  metadata:
                    environment: <+variables.environment>
                    buildNumber: ${PIPELINE_EXECUTION_ID}
                    testTypes: "unit,integration,performance"

            - step:
                name: Post-Deployment Health Check
                identifier: health_check
                type: Plugin
                spec:
                  connectorRef: prometheus
                  image: prom/prometheus:latest
                  command: |
                    # Monitor application metrics
                    promql-cli query --host ${PROMETHEUS_URL} \
                      --duration ${monitoringDuration}s \
                      --step 15s \
                      --output json \
                      'sum(rate(http_request_duration_seconds_count{env="<+variables.environment>"}[5m])) by (status_code)' > metrics.json

                    # Analyze results
                    python scripts/analyze_metrics.py \
                      --input metrics.json \
                      --error-threshold 0.01 \
                      --latency-threshold 0.5

            - step:
                name: Send Notifications
                identifier: notifications
                type: Notification
                spec:
                  type: Pipeline
                  spec:
                    users: ${NOTIFICATION_USERS}
                    userGroups: 
                      - <+variables.environment>-notifications
                    channels: <+variables.notificationChannels>
                    template:
                      subject: "Deployment to <+variables.environment> - ${PIPELINE_STATUS}"
                      body: |
                        Deployment Status: ${PIPELINE_STATUS}
                        Environment: <+variables.environment>
                        Build: ${PIPELINE_EXECUTION_ID}
                        
                        Test Results:
                        - Unit Tests: ${UNIT_TEST_STATUS}
                        - Integration Tests: ${INTEGRATION_TEST_STATUS}
                        - Load Tests: ${LOAD_TEST_STATUS}
                        
                        Security Results:
                        - OWASP ZAP: ${ZAP_STATUS}
                        - Snyk: ${SNYK_STATUS}
                        - Seeker: ${SEEKER_STATUS}
                        
                        For detailed reports, visit:
                        ${HARNESS_URL}/pipeline/${PIPELINE_ID}/execution/${PIPELINE_EXECUTION_ID}

        preDeploymentSteps:
          - step:
              name: Deployment Approval
              identifier: deployment_approval
              type: HarnessApproval
              spec:
                approvalMessage: Please review and approve deployment
                includePipelineExecutor: true
                approvers:
                  userGroups:
                    - <+variables.environment>-approvers
                  minimumCount: <+variables.minApprovers>
                  disallowPipelineExecutor: <+variables.requireApproval>
              when:
                condition: <+variables.requireApproval> == true

        rollbackSteps:
          - step:
              name: Notify Rollback
              identifier: rollback_notification
              type: Notification
              spec:
                type: Pipeline
                spec:
                  users: ${NOTIFICATION_USERS}
                  userGroups: 
                    - <+variables.environment>-notifications
                  channels: <+variables.notificationChannels>
                  template:
                    subject: "⚠️ Rollback in <+variables.environment>"
                    body: |
                      Deployment was rolled back in <+variables.environment>
                      Build: ${PIPELINE_EXECUTION_ID}
                      Reason: ${ROLLBACK_REASON}
                      
                      Please check the pipeline logs for details:
                      ${HARNESS_URL}/pipeline/${PIPELINE_ID}/execution/${PIPELINE_EXECUTION_ID}